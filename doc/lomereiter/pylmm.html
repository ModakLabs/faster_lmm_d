<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="author" content="Artem Tarasov" />
<title>[FaST-LMM] comparison with PyLMM (practice)</title>

<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  jax: ["input/TeX","output/HTML-CSS"], 
  extensions: ["tex2jax.js"], 
  tex2jax: { 
    inlineMath: [ ['$','$'], ["\\(","\\)"] ], 
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ], 
    processEscapes: true 
  }, 
}); 
</script> 
<style>
  .center-image
  {
  margin: 0 auto;
  display: block;
  width: 90%;
  }
  
pre > code {
  border: 0;
  padding-right: 0;
  padding-left: 0; }

  table{
    border-collapse: collapse;
    border: 1px solid black;
  }
  table td,th{
    border: 1px solid black;
    padding: 3px;
  }

.highlight pre code * {
  white-space: nowrap;    // this sets all children inside to nowrap
}

.highlight pre {
  overflow-x: auto;       // this sets the scrolling in x
}

.highlight pre code {
  white-space: pre;       // forces <code> to respect <pre> formatting
}

/* github style pygments theme for jekyll */
/* from https://github.com/aahan/pygments-github-style */

.highlight pre, pre, .highlight .hll { background-color: #f8f8f8; border: 1px solid #ccc; padding: 6px 10px; border-radius: 3px; }
.highlight .c { color: #999988; font-style: italic; }
.highlight .err { color: #a61717; background-color: #e3d2d2; }
.highlight .k { font-weight: bold; }
.highlight .o { font-weight: bold; }
.highlight .cm { color: #999988; font-style: italic; }
.highlight .cp { color: #999999; font-weight: bold; }
.highlight .c1 { color: #999988; font-style: italic; }
.highlight .cs { color: #999999; font-weight: bold; font-style: italic; }
.highlight .gd { color: #000000; background-color: #ffdddd; }
.highlight .gd .x { color: #000000; background-color: #ffaaaa; }
.highlight .ge { font-style: italic; }
.highlight .gr { color: #aa0000; }
.highlight .gh { color: #999999; }
.highlight .gi { color: #000000; background-color: #ddffdd; }
.highlight .gi .x { color: #000000; background-color: #aaffaa; }
.highlight .go { color: #888888; }
.highlight .gp { color: #555555; }
.highlight .gs { font-weight: bold; }
.highlight .gu { color: #800080; font-weight: bold; }
.highlight .gt { color: #aa0000; }
.highlight .kc { font-weight: bold; }
.highlight .kd { font-weight: bold; }
.highlight .kn { font-weight: bold; }
.highlight .kp { font-weight: bold; }
.highlight .kr { font-weight: bold; }
.highlight .kt { color: #445588; font-weight: bold; }
.highlight .m { color: #009999; }
.highlight .s { color: #dd1144; }
.highlight .n { color: #333333; }
.highlight .na { color: teal; }
.highlight .nb { color: #0086b3; }
.highlight .nc { color: #445588; font-weight: bold; }
.highlight .no { color: teal; }
.highlight .ni { color: purple; }
.highlight .ne { color: #990000; font-weight: bold; }
.highlight .nf { color: #990000; font-weight: bold; }
.highlight .nn { color: #555555; }
.highlight .nt { color: navy; }
.highlight .nv { color: teal; }
.highlight .ow { font-weight: bold; }
.highlight .w { color: #bbbbbb; }
.highlight .mf { color: #009999; }
.highlight .mh { color: #009999; }
.highlight .mi { color: #009999; }
.highlight .mo { color: #009999; }
.highlight .sb { color: #dd1144; }
.highlight .sc { color: #dd1144; }
.highlight .sd { color: #dd1144; }
.highlight .s2 { color: #dd1144; }
.highlight .se { color: #dd1144; }
.highlight .sh { color: #dd1144; }
.highlight .si { color: #dd1144; }
.highlight .sx { color: #dd1144; }
.highlight .sr { color: #009926; }
.highlight .s1 { color: #dd1144; }
.highlight .ss { color: #990073; }
.highlight .bp { color: #999999; }
.highlight .vc { color: teal; }
.highlight .vg { color: teal; }
.highlight .vi { color: teal; }
.highlight .il { color: #009999; }
.highlight .gc { color: #999; background-color: #EAF2F5; }

#content {
  width: 66%;
}

#list {
  width: 17%;
  vertical-align: top;
}

header {
  font-family: Sans-serif;
  font-size: 10pt;
  text-align: center;
  background-color: #cdd;
  max-width: 800px;
  border-radius: 3px;
  margin-left: auto;
  margin-right: auto;
}

section {
  max-width: 900px;
  margin-left: auto;
  margin-right: auto;
}

blockquote {
  width: 80%;
}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});

MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<header>
  <h1>[FaST-LMM] comparison with PyLMM (practice)</h1>
</header>
<table>
<tr>
  <td id="list">
  
  
    <span>March 29, 2015</span>
    <br/>
    <a href="/2015/03/29/performance.html">[LMM] literature overview: performance</a>
    <hr/>
  
  
  
    <span>March 27, 2015</span>
    <br/>
    <a href="/2015/03/27/overview.html">[LMM] literature overview: approximate methods</a>
    <hr/>
  
  
  
    <span>March 15, 2015</span>
    <br/>
    <a href="/2015/03/15/proximal.html">[FaST-LMM] Proximal contamination</a>
    <hr/>
  
  
  
    <span>March 13, 2015</span>
    <br/>
    <a href="/2015/03/13/reml.html">[FaST-LMM] REML estimate</a>
    <hr/>
  
  
  
    <span>March 11, 2015</span>
    <br/>
    <a href="/2015/03/11/mixing.html">[FaST-LMM] comparison with PyLMM (continued)</a>
    <hr/>
  
  
  
    <span>March 10, 2015</span>
    <br/>
    <a href="/2015/03/10/pylmm.html">[FaST-LMM] comparison with PyLMM (practice)</a>
    <hr/>
  
  
  
    <span>March  9, 2015</span>
    <br/>
    <a href="/2015/03/09/pylmm.html">[FaST-LMM] comparison with PyLMM (theory)</a>
    <hr/>
  
  
  
    <span>March  3, 2015</span>
    <br/>
    <a href="/2015/03/03/lmm_cov2.html">[FaST-LMM] fastlmm/inference/lmm_cov.py, part 2</a>
    <hr/>
  
  
  
    <span>February 27, 2015</span>
    <br/>
    <a href="/2015/02/27/highlevel2.html">[FaST-LMM] high-level overview, part 2</a>
    <hr/>
  
  
  
    <span>February 25, 2015</span>
    <br/>
    <a href="/2015/02/25/highlevel.html">[FaST-LMM] high-level overview of the codebase, part 1</a>
    <hr/>
  
  
  
    <span>February 18, 2015</span>
    <br/>
    <a href="/2015/02/18/lmm.html">[FaST-LMM] fastlmm/inference/lmm.py</a>
    <hr/>
  
  
  
    <span>February 16, 2015</span>
    <br/>
    <a href="/2015/02/16/lmm_cov.html">[FaST-LMM] fastlmm/inference/lmm_cov.py, part 1</a>
    <hr/>
  
  
  </td>
<td id="content">
<section>
  <h4 id="in-short-fast-lmm-computes-hatbeta-with-precomputed-k-and-its-eigendecomposition-3x-faster-on-average-than-pylmm">IN SHORT: FaST-LMM computes $\hat{\beta}$ (with precomputed $K$ and its eigendecomposition) <strong>3x</strong> faster on average than PyLMM.</h4>

<h4 id="however-with-fixed-mixing-coefficient-pylmm-computes-log-likelihood-slightly-faster-than-fast-lmm">HOWEVER: with fixed mixing coefficient PyLMM computes log-likelihood slightly faster than FaST-LMM</h4>

<hr />

<p>It turns out that, at least with <code>REML</code> parameter set to <code>True</code>, the functionality of <code>fastlmm/inference/lmm.py</code> and <code>pylmm/lmm.py</code> is almost identical. It is therefore interesting to compare the performance of the two implementations.</p>

<h2 id="loading-snps-and-kinship-matrix">Loading SNPs and kinship matrix</h2>

<p>I found PyLMM code for loading these things a bit simpler to use, and copy-pasted a few lines from <code>pylmmGWAS.py</code>.</p>

<p>For testing, I took some synthetic SNPs from <code>tests/</code> directory of FaST-LMM and ran the two tools on them. The dataset consists of 500 individuals and 4000 SNPs.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pylmm</span> <span class="kn">import</span> <span class="nb">input</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">bfile</span> <span class="o">=</span> <span class="s">&quot;/home/lomereiter/github/FaST-LMM/tests/datasets/synth/allButChr1&quot;</span>
<span class="n">pheno</span> <span class="o">=</span> <span class="s">&quot;/home/lomereiter/github/FaST-LMM/tests/datasets/synth/pheno_10_causals.txt&quot;</span>
<span class="n">kfile</span> <span class="o">=</span> <span class="s">&quot;/home/lomereiter/github/pylmm/synth_all.kin&quot;</span> <span class="c"># computed by pylmmKinship.py</span>
<span class="n">IN</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">plink</span><span class="p">(</span><span class="n">bfile</span><span class="p">,</span><span class="nb">type</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="n">phenoFile</span><span class="o">=</span><span class="n">pheno</span><span class="p">,</span><span class="n">normGenotype</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">IN</span><span class="o">.</span><span class="n">phenos</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">kfile</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">),</span><span class="n">sep</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">)</span>
<span class="n">K</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">IN</span><span class="o">.</span><span class="n">indivs</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">IN</span><span class="o">.</span><span class="n">indivs</span><span class="p">)))</span></code></pre></div>

<h2 id="preparing-lmm-aka-eigendecomposition-of--k-">Preparing LMM (aka eigendecomposition of $ K $)</h2>

<p>Each took roughly 0.3 seconds on my laptop. Both tools use <code>scipy.linalg.eigh</code> under the hood.</p>

<h3 id="pylmm">PyLMM</h3>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pylmm.lmm</span> <span class="kn">as</span> <span class="nn">py</span>

<span class="k">def</span> <span class="nf">preparePyLMM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">py</span><span class="o">.</span><span class="n">LMM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span></code></pre></div>

<h3 id="fast-lmm">FaST-LMM</h3>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">fastlmm.inference.lmm</span> <span class="kn">as</span> <span class="nn">fast</span>

<span class="k">def</span> <span class="nf">prepareFastLMM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="n">lmm</span> <span class="o">=</span> <span class="n">fast</span><span class="o">.</span><span class="n">LMM</span><span class="p">()</span>
    <span class="n">lmm</span><span class="o">.</span><span class="n">setK</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">lmm</span><span class="o">.</span><span class="n">sety</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lmm</span></code></pre></div>

<h2 id="computing-hatbeta-given-x-covariates">Computing $\hat{\beta}$ given $X$ (covariates)</h2>

<p>After LMM initialization, we loop over the SNPs:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">snp</span><span class="p">,</span><span class="nb">id</span> <span class="ow">in</span> <span class="n">IN</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">snp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">snp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="o">...</span></code></pre></div>

<p>In the loop, <code>x</code> is the tested SNP, which is one column of the covariates matrix (the other is vector of ones).</p>

<h3 id="pylmm-1">PyLMM</h3>

<p>Adds the column of ones automatically.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">computeBetaPyLMM</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">REML</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">lmm</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">lmm</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">lmm</span> <span class="o">=</span> <span class="n">preparePyLMM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
    <span class="n">hmax</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">lmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">REML</span><span class="o">=</span><span class="n">REML</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">hmax</span></code></pre></div>

<h3 id="fast-lmm-1">FaST-LMM</h3>

<p>A bit more of manual work is required, but the interface is still reasonably simple. For accurate measurements, I excluded the time spent in calls to <code>np.ones</code> and <code>np.hstack</code>.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">computeBetaFastLMM</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">REML</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">lmm</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">lmm</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="n">lmm</span> <span class="o">=</span> <span class="n">prepareFastLMM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
    <span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">X0</span><span class="p">))</span>
    <span class="n">lmm</span><span class="o">.</span><span class="n">setX</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">lmm</span><span class="o">.</span><span class="n">findH2</span><span class="p">(</span><span class="n">REML</span><span class="o">=</span><span class="n">REML</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h2</span><span class="p">[</span><span class="s">&#39;beta&#39;</span><span class="p">],</span> <span class="n">h2</span><span class="p">[</span><span class="s">&#39;sigma2&#39;</span><span class="p">],</span> <span class="n">h2</span><span class="p">[</span><span class="s">&#39;h2&#39;</span><span class="p">]</span></code></pre></div>

<h2 id="the-loop">The loop</h2>

<p>Here’s the complete loop, where I added a simple decorator <code>timeit</code> that measures the time spent in the function.</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">snp</span><span class="p">,</span><span class="nb">id</span> <span class="ow">in</span> <span class="n">IN</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">snp</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">snp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">beta</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">mix</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">(</span><span class="n">computeBetaPyLMM</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">REML</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">lmm</span><span class="o">=</span><span class="n">lmm_py</span><span class="p">)</span>
    <span class="n">beta2</span><span class="p">,</span> <span class="n">var2</span><span class="p">,</span> <span class="n">mix2</span><span class="p">,</span> <span class="n">t2</span> <span class="o">=</span> <span class="n">timeit</span><span class="p">(</span><span class="n">computeBetaFastLMM</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">REML</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">lmm</span><span class="o">=</span><span class="n">lmm_fast</span><span class="p">)</span></code></pre></div>

<p>I also included checks that the results are indeed the same, and printed the times into a file:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#####################################################</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">var</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">var2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">mix</span> <span class="o">-</span> <span class="n">mix2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;{0}</span><span class="se">\t</span><span class="s">{1}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">t2</span><span class="p">))</span></code></pre></div>

<h2 id="the-results">The results</h2>

<p>FaST-LMM has computed $\hat{\beta}$ on average <strong>3x</strong> faster than PyLMM. Below is the density plot: the distributions don’t even intersect.</p>

<p>Both distributions are multimodal for some reason, the time depends on the $X$ matrix in a complicated way.</p>

<!---

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kn">library</span><span class="p">(</span>ggplot2<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>reshape2<span class="p">)</span>

timings <span class="o">&lt;-</span> read.table<span class="p">(</span><span class="s">&quot;timings.txt&quot;</span><span class="p">)</span>
<span class="kp">colnames</span><span class="p">(</span>timings<span class="p">)</span> <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;pylmm&quot;</span><span class="p">,</span> <span class="s">&quot;fastlmm&quot;</span><span class="p">)</span>
timings.melt <span class="o">&lt;-</span> melt<span class="p">(</span>timings<span class="p">,</span> variable.name<span class="o">=</span><span class="s">&quot;method&quot;</span><span class="p">,</span> value.name<span class="o">=</span><span class="s">&quot;time&quot;</span><span class="p">)</span>
ggplot<span class="p">(</span>timings.melt<span class="p">)</span> <span class="o">+</span> geom_density<span class="p">(</span>aes<span class="p">(</span>x<span class="o">=</span>time<span class="p">,</span> fill<span class="o">=</span>method<span class="p">))</span> <span class="o">+</span> 
		     scale_x_continuous<span class="p">(</span>limits<span class="o">=</span><span class="kt">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">0.05</span><span class="p">))</span> <span class="o">+</span> 
		     xlab<span class="p">(</span><span class="kp">expression</span><span class="p">(</span><span class="s">&quot;time for estimating &quot;</span><span class="o">*</span> <span class="kp">beta</span><span class="o">*</span> <span class="s">&quot;, seconds&quot;</span><span class="p">))</span></code></pre></div>

-->

<p><img src="2015-03-10-beta.png" alt="" class="center-image" /></p>

<p>Finally, here’s the distribution of the ratio <script type="math/tex">\dfrac{\textrm{PyLMM time}}{\textrm{FaST-LMM time}}</script> on the 4000 SNPs (why it’s bimodal, I have no idea):</p>

<p><img src="2015-03-10-ratio.png" alt="" class="center-image" /></p>

<h2 id="computations-with-fixed-mixing-coefficient">Computations with fixed mixing coefficient</h2>

<p>In the above discussion, most of the time is spent on finding optimal <code>h2</code> value. The reasonable questions to ask are:</p>

<ul>
  <li>Does it change a lot?</li>
  <li>How do the tools perform when the parameter is fixed?</li>
</ul>

<h3 id="distribution-of-h2">Distribution of <code>h2</code></h3>

<p>As can be seen, it doesn’t change that much, considering that its range is potentially the whole $(0, 1)$. </p>

<p>It’s therefore justified to use the same value for all calculations, optimized for the null model ($X = \mathbf{1}_n$) — even if there is a loss in statistical power, it doesn’t overweigh the 10x speedup.</p>

<p><img src="2015-03-10-h2.png" alt="" class="center-image" /></p>

<h3 id="performance-of-log-likelihood-calculation">Performance of log-likelihood calculation</h3>

<p>Surprisingly, PyLMM computes log-likelihood a little faster than FaST-LMM:</p>

<p><img src="2015-03-10-beta2.png" alt="" class="center-image" /></p>

<p>Some of the choices made by FaST-LMM developers, are debatable. For example, here they could use <code>NP.slogdet(XX)</code> instead of taking the decomposition (neither eigenvalues nor eigenvectors are used later in the code):</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">XX</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
<span class="p">[</span><span class="n">Sxx</span><span class="p">,</span><span class="n">Uxx</span><span class="p">]</span><span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span>
<span class="n">logdetXX</span>  <span class="o">=</span> <span class="n">SP</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Sxx</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span></code></pre></div>

<p>Another debatable choice is taking eigendecomposition of <code>XKX</code> where its (generalized) inverse is needed.
PyLMM is faster here at the expense of ignoring degenerate cases. It simply takes <code>linalg.inv</code> which internally uses a (pivoted) LU-decomposition. Given that it didn’t fail on the test dataset, it makes me think that it’s more reasonable to use SVD- or eigendecomposition only as a fallback, when LU indicates that the matrix is singular.</p>

<p><strong>UPD (11.03.2015):</strong> even better option is to use Cholesky decomposition, since the matrix is positive (semi?)definite. I checked, and when used instead of <code>linalg.inv</code>, that cuts off 10% of <code>pylmm.lmm.LMM.LL</code> running time.</p>

</section>
</td></tr></table>

</div>
</div>
</body>
</html>
